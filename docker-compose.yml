version: '3.6'

services:
  llama-gpt-api-7b:
    build:
      context: ./api
      dockerfile: Dockerfile
    restart: on-failure
    environment:
      MODEL: '/models/llama-2-7b-chat.bin'

  llama-gpt-ui:
    build:
      context: ./ui # Change this to the correct path for your Next.js app
      dockerfile: Dockerfile # Ensure you have a Dockerfile in the UI directory
    ports:
      - 3000:3000
    restart: on-failure
    environment:
      - 'OPENAI_API_KEY=sk-XXXXXXXXXXXXXXXXXXXX'
      - 'OPENAI_API_HOST=http://llama-gpt-api-7b:8000'
      - 'DEFAULT_MODEL=/models/llama-2-7b-chat.bin'
      - 'WAIT_HOSTS=llama-gpt-api-7b:8000'
      - 'WAIT_TIMEOUT=600'
