version: '3.9'

networks:
  net:
    driver: bridge

services:
  chroma-server:
    image: server
    build:
      context: ../chroma
      dockerfile: Dockerfile
    volumes:
      - ../chroma:/chroma
      - index_data:/index_data
    command: uvicorn chromadb.app:app --reload --workers 1 --host 0.0.0.0 --port 8000 --log-config log_config.yml
    environment:
      - IS_PERSISTENT=TRUE
    ports:
      - 8000:8000
    networks:
      - net

  llama-gpt-api-70b:
    build:
      context: ./api
      dockerfile: 70B.Dockerfile
    restart: on-failure
    environment:
      MODEL: '/models/llama-2-70b-chat.bin'
      # Llama 2 70B's grouping factor is 8 compared to 7B and 13B's 1. Currently,
      # it's not possible to change this using --n_gqa with llama-cpp-python in
      # run.sh, so we expose it as an environment variable.
      # See: https://github.com/abetlen/llama-cpp-python/issues/528
      # and: https://github.com/facebookresearch/llama/issues/407
      N_GQA: '8'
      USE_MLOCK: 1
    cap_add:
      - IPC_LOCK
    networks:
      - net

  llama-gpt-ui:
    build:
      context: ./ui 
      dockerfile: Dockerfile 
    ports:
      - 3000:3000
    restart: on-failure
    environment:
      - 'OPENAI_API_KEY=sk-XXXXXXXXXXXXXXXXXXXX'
      - 'OPENAI_API_HOST=http://llama-gpt-api-70b:8000'
      - 'DEFAULT_MODEL=/models/llama-2-70b-chat.bin'
      - 'WAIT_HOSTS=llama-gpt-api-70b:8000'
      - 'WAIT_TIMEOUT=600'
    networks:
      - net

volumes:
  index_data:
    driver: local
  backups:
    driver: local
